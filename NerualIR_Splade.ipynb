{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IwelM4d4fWgX",
    "ExecuteTime": {
     "end_time": "2024-10-24T17:30:57.955303Z",
     "start_time": "2024-10-24T17:30:57.937285Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_id = 'naver/splade_v2_max'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ],
   "metadata": {
    "id": "6V0d_awCfcYC",
    "ExecuteTime": {
     "end_time": "2024-10-24T17:31:18.381039Z",
     "start_time": "2024-10-24T17:31:08.632298Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/258 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5816e59970064bd595d5bb4d17100b4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/488 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad74e03b49b54e9f83beec5bbfe9867f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0deb37e83324e12b586aa5d02d1c5a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80491a9c0c1241a5b8fdf7cd88f45316"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76d843469a544b9ca23683e633514ebd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"Information Retrieval course at the University of Southern Maine\"\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "output = model(**tokens)\n",
    "print(output)\n",
    "print(output.logits.shape)\n",
    "import torch\n",
    "\n",
    "vec = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens.attention_mask.unsqueeze(-1),\n",
    "dim=1)[0].squeeze()\n",
    "# Get SPLADE Vector\n",
    "print(vec.shape) #torch.Size([30522])\n"
   ],
   "metadata": {
    "id": "gUs0sMVRfeos",
    "ExecuteTime": {
     "end_time": "2024-10-24T17:31:24.690919Z",
     "start_time": "2024-10-24T17:31:24.573345Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[ -8.6903,  -8.6712,  -8.7282,  ...,  -7.7577,  -7.6470,  -8.4183],\n",
      "         [-12.9130, -12.7483, -12.8099,  ..., -11.4523, -11.3861, -12.9494],\n",
      "         [-10.7632, -10.4263, -10.6957,  ...,  -9.3885,  -9.1583, -12.1823],\n",
      "         ...,\n",
      "         [-13.8963, -14.0878, -14.1625,  ..., -11.4822, -12.5815, -12.8123],\n",
      "         [-12.7274, -12.8395, -12.9904,  ..., -11.2488, -11.8553, -10.9621],\n",
      "         [-10.6878, -10.6928, -10.7399,  ...,  -8.6930,  -8.7811,  -9.7368]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 11, 30522])\n",
      "torch.Size([30522])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# extract non-zero positions\n",
    "cols = vec.nonzero().squeeze().cpu().tolist()\n",
    "print(len(cols))\n",
    "\n",
    "# extract the non-zero values\n",
    "weights = vec[cols].cpu().tolist()\n",
    "# use to create a dictionary of token ID to weight\n",
    "sparse_dict = dict(zip(cols, weights))\n",
    "sparse_dict\n",
    "\n",
    "# extract the ID position to text token mappings\n",
    "idx2token = {\n",
    "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
    "}\n"
   ],
   "metadata": {
    "id": "EhKpOeJIfsQc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# map token IDs to human-readable tokens\n",
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
    "}\n",
    "# sort so we can see most relevant tokens first\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens\n"
   ],
   "metadata": {
    "id": "te_xwRZ3fuu7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "text1 = \"Information Retrieval course at the University of Southern Maine\"\n",
    "text2 = \"Courses about search engines in Maine\"\n",
    "text3 = \"Courses to avoid in computer science\"\n",
    "tokens1 = tokenizer(text1, return_tensors='pt')\n",
    "tokens2 = tokenizer(text2, return_tensors='pt')\n",
    "tokens3 = tokenizer(text3, return_tensors='pt')\n",
    "\n",
    "output = model(**tokens1)\n",
    "vec1 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens1.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
    "\n",
    "output = model(**tokens2)\n",
    "vec2 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens2.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
    "\n",
    "output = model(**tokens3)\n",
    "vec3 = torch.max(torch.log(1 + torch.relu(output.logits)\n",
    "    ) * tokens3.attention_mask.unsqueeze(-1),dim=1)[0].squeeze().view(-1, 1)\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "output = cos(vec1, vec2)\n",
    "print(output)\n",
    "output = cos(vec1, vec3)\n",
    "print(output)"
   ],
   "metadata": {
    "id": "vZSUj1AHfv9u"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
