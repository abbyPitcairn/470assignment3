{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T17:32:43.998639Z",
     "start_time": "2024-10-24T17:32:22.724408Z"
    },
    "id": "yZlGL_S6AQMU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6366]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"Information Retrieval course at the University of Southern Maine for computer scientist.\")\n",
    "emb2 = model.encode(\"Best computer science course.\")\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C5cXDMNgEEcQ"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'topics_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m## reading queries and collection\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m dic_topics \u001b[38;5;241m=\u001b[39m \u001b[43mload_topic_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopics_1.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m queries \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_id \u001b[38;5;129;01min\u001b[39;00m dic_topics:\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mload_topic_file\u001b[0;34m(topic_filepath)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_topic_file\u001b[39m(topic_filepath):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# a method used to read the topic file for this year of the lab; to be passed to BERT/PyTerrier methods\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     queries \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtopic_filepath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m     result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m     37\u001b[0m       \u001b[38;5;66;03m# You may do additional preprocessing here\u001b[39;00m\n\u001b[1;32m     38\u001b[0m       \u001b[38;5;66;03m# returing results as dictionary of topic id: [title, body, tag]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'topics_1.json'"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Cross-encoder\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import string\n",
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder, losses\n",
    "import torch\n",
    "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator, CEBinaryClassificationEvaluator, \\\n",
    "    CERerankingEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "\n",
    "def read_qrel_file(qrel_filepath):\n",
    "    # a method used to read the topic file\n",
    "    result = {}\n",
    "    with open(qrel_filepath, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', lineterminator='\\n')\n",
    "        for line in reader:\n",
    "            query_id = line[0]\n",
    "            doc_id = line[2]\n",
    "            score = int(line[3])\n",
    "            if query_id in result:\n",
    "                result[query_id][doc_id] = score\n",
    "            else:\n",
    "                result[query_id] = {doc_id: score}\n",
    "    # dictionary of key:query_id value: dictionary of key:doc id value: score\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_topic_file(topic_filepath):\n",
    "    # a method used to read the topic file for this year of the lab; to be passed to BERT/PyTerrier methods\n",
    "    queries = json.load(open(topic_filepath))\n",
    "    result = {}\n",
    "    for item in queries:\n",
    "      # You may do additional preprocessing here\n",
    "      # returing results as dictionary of topic id: [title, body, tag]\n",
    "      title = item['Title'].translate(str.maketrans('', '', string.punctuation))\n",
    "      body = item['Body'].translate(str.maketrans('', '', string.punctuation))\n",
    "      tags = item['Tags']\n",
    "      result[item['Id']] = [title, body, tags]\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_collection(answer_filepath):\n",
    "  # Reading collection to a dictionary\n",
    "  lst = json.load(open(answer_filepath))\n",
    "  result = {}\n",
    "  for doc in lst:\n",
    "    result[doc['Id']] = doc['Text']\n",
    "  return result\n",
    "\n",
    "\n",
    "## reading queries and collection\n",
    "dic_topics = load_topic_file(\"topics_1.json\")\n",
    "queries = {}\n",
    "for query_id in dic_topics:\n",
    "    queries[query_id] = \"[TITLE]\" + dic_topics[query_id][0] + \"[BODY]\" + dic_topics[query_id][1]\n",
    "qrel = read_qrel_file(\"qrel_1.tsv\")\n",
    "collection_dic = read_collection('Answers.json')\n",
    "\n",
    "## Preparing pairs of training instances\n",
    "num_topics = len(queries.keys())\n",
    "number_training_samples = int(num_topics*0.9)\n",
    "\n",
    "\n",
    "## Preparing the content\n",
    "counter = 1\n",
    "train_samples = []\n",
    "valid_samples = {}\n",
    "for qid in qrel:\n",
    "    # key: doc id, value: relevance score\n",
    "    dic_doc_id_relevance = qrel[qid]\n",
    "    # query text\n",
    "    topic_text = queries[qid]\n",
    "\n",
    "    if counter < number_training_samples:\n",
    "        for doc_id in dic_doc_id_relevance:\n",
    "            label = dic_doc_id_relevance[doc_id]\n",
    "            content = collection_dic[doc_id]\n",
    "            if label >= 1:\n",
    "                label = 1\n",
    "            train_samples.append(InputExample(texts=[topic_text, content], label=label))\n",
    "    else:\n",
    "        for doc_id in dic_doc_id_relevance:\n",
    "            label = dic_doc_id_relevance[doc_id]\n",
    "            if qid not in valid_samples:\n",
    "                valid_samples[qid] = {'query': topic_text, 'positive': set(), 'negative': set()}\n",
    "            if label == 0:\n",
    "                label = 'negative'\n",
    "            else:\n",
    "                label = 'positive'\n",
    "            content = collection_dic[doc_id]\n",
    "            valid_samples[qid][label].add(content)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Training and validation set prepared\")\n",
    "\n",
    "# selecting cross-encoder\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "# Learn how to use GPU with this!\n",
    "model = CrossEncoder(model_name)\n",
    "\n",
    "# Adding special tokens\n",
    "tokens = [\"[TITLE]\", \"[BODY]\"]\n",
    "model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "model.model.resize_token_embeddings(len(model.tokenizer))\n",
    "\n",
    "num_epochs = 2\n",
    "model_save_path = \"./ft_cr_2024\"\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=4)\n",
    "# During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
    "evaluator = CERerankingEvaluator(valid_samples, name='train-eval')\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% of train data for warm-up\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          save_best_model=True)\n",
    "\n",
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "052gC8QAruIi"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning Bi-encoder\n",
    "# Models: https://sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import islice\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "def read_qrel_file(file_path):\n",
    "    # Reading the qrel file\n",
    "    dic_topic_id_answer_id_relevance = {}\n",
    "    with open(file_path) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            topic_id = row[0]\n",
    "            answer_id = int(row[2])\n",
    "            relevance_score = int(row[3])\n",
    "            if topic_id in dic_topic_id_answer_id_relevance:\n",
    "                dic_topic_id_answer_id_relevance[topic_id][answer_id] = relevance_score\n",
    "            else:\n",
    "                dic_topic_id_answer_id_relevance[topic_id] = {answer_id: relevance_score}\n",
    "    return dic_topic_id_answer_id_relevance\n",
    "\n",
    "\n",
    "def load_topic_file(topic_filepath):\n",
    "    # a method used to read the topic file for this year of the lab; to be passed to BERT/PyTerrier methods\n",
    "    queries = json.load(open(topic_filepath))\n",
    "    result = {}\n",
    "    for item in queries:\n",
    "      # You may do additional preprocessing here\n",
    "      # returing results as dictionary of topic id: [title, body, tag]\n",
    "      title = item['Title'].translate(str.maketrans('', '', string.punctuation))\n",
    "      body = item['Body'].translate(str.maketrans('', '', string.punctuation))\n",
    "      tags = item['Tags']\n",
    "      result[item['Id']] = [title, body, tags]\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_collection(answer_filepath):\n",
    "  # Reading collection to a dictionary\n",
    "  lst = json.load(open(answer_filepath))\n",
    "  result = {}\n",
    "  for doc in lst:\n",
    "    result[int(doc['Id'])] = doc['Text']\n",
    "  return result\n",
    "\n",
    "\n",
    "# Uses the posts file, topic file(s) and qrel file(s) to build our training and evaluation sets.\n",
    "def process_data(queries, train_dic_qrel, val_dic_qrel, collection_dic):\n",
    "    train_samples = []\n",
    "    evaluator_samples_1 = []\n",
    "    evaluator_samples_2 = []\n",
    "    evaluator_samples_score = []\n",
    "\n",
    "    # Build Training set\n",
    "    for topic_id in train_dic_qrel:\n",
    "        question = queries[topic_id]\n",
    "        dic_answer_id = train_dic_qrel.get(topic_id, {})\n",
    "\n",
    "        for answer_id in dic_answer_id:\n",
    "            score = dic_answer_id[answer_id]\n",
    "            answer = collection_dic[answer_id]\n",
    "            if score > 1:\n",
    "                train_samples.append(InputExample(texts=[question, answer], label=1.0))\n",
    "            else:\n",
    "                train_samples.append(InputExample(texts=[question, answer], label=0.0))\n",
    "    for topic_id in val_dic_qrel:\n",
    "        question = queries[topic_id]\n",
    "        dic_answer_id = val_dic_qrel.get(topic_id, {})\n",
    "\n",
    "        for answer_id in dic_answer_id:\n",
    "            score = dic_answer_id[answer_id]\n",
    "            answer = collection_dic[answer_id]\n",
    "            if score > 1:\n",
    "                label = 1.0\n",
    "            elif score == 1:\n",
    "                label = 0.5\n",
    "            else:\n",
    "                label = 0.0\n",
    "            evaluator_samples_1.append(question)\n",
    "            evaluator_samples_2.append(answer)\n",
    "            evaluator_samples_score.append(label)\n",
    "\n",
    "    return train_samples, evaluator_samples_1, evaluator_samples_2, evaluator_samples_score\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_dict(d):\n",
    "    keys = list(d.keys())\n",
    "    random.shuffle(keys)\n",
    "    return {key: d[key] for key in keys}\n",
    "\n",
    "\n",
    "def split_train_validation(qrels, ratio=0.9):\n",
    "    # Using items() + len() + list slicing\n",
    "    # Split dictionary by half\n",
    "    n = len(qrels)\n",
    "    n_split = int(n * ratio)\n",
    "    qrels = shuffle_dict(qrels)\n",
    "    train = dict(islice(qrels.items(), n_split))\n",
    "    validation = dict(islice(qrels.items(), n_split, None))\n",
    "\n",
    "    return train, validation\n",
    "\n",
    "\n",
    "def train(model):\n",
    "\n",
    "    ## reading queries and collection\n",
    "    dic_topics = load_topic_file(\"topics_1.json\")\n",
    "    queries = {}\n",
    "    for query_id in dic_topics:\n",
    "        queries[query_id] = \"[TITLE]\" + dic_topics[query_id][0] + \"[BODY]\" + dic_topics[query_id][1]\n",
    "    qrel = read_qrel_file(\"qrel_1.tsv\")\n",
    "    collection_dic = read_collection('Answers.json')\n",
    "    train_dic_qrel, val_dic_qrel = split_train_validation(qrel)\n",
    "\n",
    "    # print(train_dic_qrel)\n",
    "    # print(val_dic_qrel)\n",
    "\n",
    "    num_epochs = 100\n",
    "    batch_size = 16\n",
    "\n",
    "    # Rename this when training the model and keep track of results\n",
    "    MODEL = \"SAVED_MODEL_NAME\"\n",
    "\n",
    "    # Creating train and val dataset\n",
    "    train_samples, evaluator_samples_1, evaluator_samples_2, evaluator_samples_score = process_data(queries, train_dic_qrel, val_dic_qrel, collection_dic)\n",
    "\n",
    "    train_dataset = SentencesDataset(train_samples, model=model)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size=batch_size)\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    evaluator = evaluation.EmbeddingSimilarityEvaluator(evaluator_samples_1, evaluator_samples_2, evaluator_samples_score, write_csv=\"evaluation-epoch.csv\")\n",
    "    warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "    # add evaluator to the model fit function\n",
    "    model.fit(\n",
    "        train_objectives =[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=num_epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        use_amp=True,\n",
    "        save_best_model=True,\n",
    "        show_progress_bar=True,\n",
    "        output_path=MODEL\n",
    "    )\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "train(model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
